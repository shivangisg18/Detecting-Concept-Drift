{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c13cf1c3-ae10-4960-992e-6748447a34cf",
   "metadata": {},
   "source": [
    "## Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c23fa193-08a3-4fed-a487-f2c89443e048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded datasets:\n",
      "(50000, 32) (60, 14) (480, 8) (1000, 20) (82, 18)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'upload_date'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'upload_date'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m     main[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupload_date\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(main[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupload_date\u001b[39m\u001b[38;5;124m\"\u001b[39m], errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Extract year, month for merging\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m main[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m main[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupload_date\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mto_period(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# ===============================================================\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# 3. MERGING DATASETS\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# ===============================================================\u001b[39;00m\n\u001b[1;32m     53\u001b[0m df \u001b[38;5;241m=\u001b[39m main\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pandas/core/frame.py:4113\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4113\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[1;32m   4114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4115\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/pandas/core/indexes/base.py:3819\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3815\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3816\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3817\u001b[0m     ):\n\u001b[1;32m   3818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3820\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3821\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3822\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3823\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3824\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'upload_date'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# ===============================================================\n",
    "# 1. LOAD ALL DATASETS\n",
    "# ===============================================================\n",
    "\n",
    "path = '/Users/satvikshankar/Downloads/archive (2)/'\n",
    "\n",
    "main = pd.read_csv(path + \"youtube_shorts_tiktok_trends_2025.csv_ML.csv\")\n",
    "country = pd.read_csv(path + \"country_platform_summary_2025.csv\")\n",
    "monthly = pd.read_csv(path + \"monthly_trends_2025.csv\")\n",
    "creators = pd.read_csv(path + \"top_creators_impact_2025.csv\")\n",
    "hashtags = pd.read_csv(path + \"top_hashtags_2025.csv\")\n",
    "\n",
    "print(\"Loaded datasets:\")\n",
    "print(main.shape, country.shape, monthly.shape, creators.shape, hashtags.shape)\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 2. BASIC CLEANING & TYPE FIXES\n",
    "# ===============================================================\n",
    "\n",
    "# Standardize column names (lowercase, snake_case)\n",
    "def normalize_cols(df):\n",
    "    df.columns = (\n",
    "        df.columns.str.strip()\n",
    "        .str.lower()\n",
    "        .str.replace(\" \", \"_\")\n",
    "        .str.replace(\"-\", \"_\")\n",
    "    )\n",
    "    return df\n",
    "\n",
    "main = normalize_cols(main)\n",
    "country = normalize_cols(country)\n",
    "monthly = normalize_cols(monthly)\n",
    "creators = normalize_cols(creators)\n",
    "hashtags = normalize_cols(hashtags)\n",
    "\n",
    "# Convert upload_date to datetime\n",
    "if \"upload_date\" in main.columns:\n",
    "    main[\"upload_date\"] = pd.to_datetime(main[\"upload_date\"], errors=\"coerce\")\n",
    "\n",
    "# Extract year, month for merging\n",
    "main[\"month\"] = main[\"upload_date\"].dt.to_period(\"M\").astype(str)\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 3. MERGING DATASETS\n",
    "# ===============================================================\n",
    "\n",
    "df = main.copy()\n",
    "\n",
    "# Merge with country-level summary\n",
    "if {\"country\", \"platform\"}.issubset(df.columns) and {\"country\", \"platform\"}.issubset(country.columns):\n",
    "    df = df.merge(country, on=[\"country\", \"platform\"], how=\"left\")\n",
    "\n",
    "# Merge with monthly trends\n",
    "if \"month\" in df.columns and \"month\" in monthly.columns:\n",
    "    df = df.merge(monthly, on=\"month\", how=\"left\")\n",
    "\n",
    "# Merge with top creators (if creator_name exists)\n",
    "creator_key = \"creator_name\"\n",
    "if creator_key in df.columns and creator_key in creators.columns:\n",
    "    df = df.merge(creators, on=creator_key, how=\"left\")\n",
    "\n",
    "# Merge hashtags (explode if multiple hashtags in a list)\n",
    "if \"hashtag\" in hashtags.columns and \"hashtags\" in df.columns:\n",
    "    df[\"hashtags\"] = df[\"hashtags\"].fillna(\"\").astype(str)\n",
    "    df_exp = df.assign(hashtag=df[\"hashtags\"].str.split(\",\")).explode(\"hashtag\")\n",
    "    df = df_exp.merge(hashtags, on=\"hashtag\", how=\"left\").groupby(\"video_id\").first().reset_index()\n",
    "\n",
    "print(\"Merged dataset shape:\", df.shape)\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 4. HANDLE MISSING VALUES\n",
    "# ===============================================================\n",
    "\n",
    "# Drop rows missing key identifiers\n",
    "required_cols = [\"video_id\", \"platform\", \"upload_date\"]\n",
    "df = df.dropna(subset=[c for c in required_cols if c in df.columns])\n",
    "\n",
    "# Fill numeric columns\n",
    "num_cols = df.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "df[num_cols] = df[num_cols].fillna(df[num_cols].median())\n",
    "\n",
    "# Fill categorical columns\n",
    "cat_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "df[cat_cols] = df[cat_cols].fillna(\"Unknown\")\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 5. REMOVE DUPLICATES\n",
    "# ===============================================================\n",
    "\n",
    "if {\"video_id\", \"platform\"}.issubset(df.columns):\n",
    "    df = df.drop_duplicates(subset=[\"video_id\", \"platform\"])\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 6. OUTLIER HANDLING (IQR method)\n",
    "# ===============================================================\n",
    "\n",
    "for col in num_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    df[col] = np.where(df[col] < lower, lower, df[col])\n",
    "    df[col] = np.where(df[col] > upper, upper, df[col])\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 7. NORMALIZE ENGAGEMENT METRICS\n",
    "# ===============================================================\n",
    "\n",
    "eng_cols = [c for c in df.columns if c in [\"views\", \"likes\", \"shares\", \"comments\"]]\n",
    "\n",
    "for col in eng_cols:\n",
    "    df[col + \"_per_view\"] = df[col] / (df[\"views\"] + 1) if \"views\" in df.columns else df[col]\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 8. FEATURE ENCODING\n",
    "# ===============================================================\n",
    "\n",
    "# One-hot encode key categories\n",
    "onehot_cols = [\"platform\", \"language\", \"category\", \"sound_type\", \"country\"]\n",
    "onehot_cols = [c for c in onehot_cols if c in df.columns]\n",
    "\n",
    "df = pd.get_dummies(df, columns=onehot_cols, drop_first=True)\n",
    "\n",
    "# Label encode smaller categories\n",
    "if \"trend_label\" in df.columns:\n",
    "    le = LabelEncoder()\n",
    "    df[\"trend_label_encoded\"] = le.fit_transform(df[\"trend_label\"])\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 9. STANDARD SCALING FOR NUMERIC FEATURES\n",
    "# ===============================================================\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaled_cols = df.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "df[scaled_cols] = scaler.fit_transform(df[scaled_cols])\n",
    "\n",
    "print(\"Final preprocessed dataset shape:\", df.shape)\n",
    "\n",
    "# Your final dataset:\n",
    "final_df = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6abe405-7ac2-461f-9a6f-11314902beb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded datasets:\n",
      "(50000, 32) (60, 14) (480, 8) (1000, 20) (82, 18)\n",
      "Merged dataset shape: (1400094, 52)\n",
      "Dropping zero-variance columns: ['weekend_hashtag_boost', 'total_videos', 'total_views', 'median_er', 'p95_views', 'avg_duration', 'avg_velocity_x', 'avg_comment_ratio', 'avg_share_rate', 'avg_save_rate', 'avg_engagement_per_1k', 'top_hashtag_views']\n",
      "Final preprocessed dataset shape: (1400094, 76)\n",
      "Saved preprocessed dataset to: /Users/satvikshankar/Downloads/youtube_tiktok_trends_2025_preprocessed.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# ===============================================================\n",
    "# 1. LOAD ALL DATASETS\n",
    "# ===============================================================\n",
    "\n",
    "path = '/Users/satvikshankar/Downloads/archive (2)/'\n",
    "\n",
    "main = pd.read_csv(path + \"youtube_shorts_tiktok_trends_2025.csv_ML.csv\")\n",
    "country = pd.read_csv(path + \"country_platform_summary_2025.csv\")\n",
    "monthly = pd.read_csv(path + \"monthly_trends_2025.csv\")\n",
    "creators = pd.read_csv(path + \"top_creators_impact_2025.csv\")\n",
    "hashtags = pd.read_csv(path + \"top_hashtags_2025.csv\")\n",
    "\n",
    "print(\"Loaded datasets:\")\n",
    "print(main.shape, country.shape, monthly.shape, creators.shape, hashtags.shape)\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 2. BASIC CLEANING & TYPE FIXES\n",
    "# ===============================================================\n",
    "\n",
    "def normalize_cols(df):\n",
    "    df.columns = (\n",
    "        df.columns.str.strip()\n",
    "        .str.lower()\n",
    "        .str.replace(\" \", \"_\")\n",
    "        .str.replace(\"-\", \"_\")\n",
    "    )\n",
    "    return df\n",
    "\n",
    "main = normalize_cols(main)\n",
    "country = normalize_cols(country)\n",
    "monthly = normalize_cols(monthly)\n",
    "creators = normalize_cols(creators)\n",
    "hashtags = normalize_cols(hashtags)\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 3. MERGING DATASETS (NO TIMESTAMP MERGE)\n",
    "# ===============================================================\n",
    "\n",
    "df = main.copy()\n",
    "\n",
    "# Merge with country summary if possible\n",
    "if {\"region\", \"platform\"}.issubset(df.columns) and {\"country\", \"platform\"}.issubset(country.columns):\n",
    "    df = df.merge(country, left_on=[\"region\", \"platform\"], right_on=[\"country\", \"platform\"], how=\"left\")\n",
    "\n",
    "# Merge monthly trends ONLY if \"trend_label\" matches monthly trend category\n",
    "if \"trend_label\" in df.columns and \"trend_label\" in monthly.columns:\n",
    "    df = df.merge(monthly, on=\"trend_label\", how=\"left\")\n",
    "\n",
    "# Merge creators if matching key exists\n",
    "if \"creator_tier\" in df.columns and \"creator_tier\" in creators.columns:\n",
    "    df = df.merge(creators, on=\"creator_tier\", how=\"left\")\n",
    "\n",
    "# Merge hashtags if you have hashtag interaction metrics\n",
    "if \"hashtag\" in hashtags.columns:\n",
    "    # This dataset has no raw hashtags column so skip\n",
    "    pass\n",
    "\n",
    "print(\"Merged dataset shape:\", df.shape)\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 4. HANDLE MISSING VALUES\n",
    "# ===============================================================\n",
    "\n",
    "# Drop rows missing minimal keys\n",
    "required_cols = [\"platform\", \"category\"]\n",
    "df = df.dropna(subset=[c for c in required_cols if c in df.columns])\n",
    "\n",
    "# Numeric fill\n",
    "num_cols = df.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "df[num_cols] = df[num_cols].fillna(df[num_cols].median())\n",
    "\n",
    "# Categorical fill\n",
    "cat_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "df[cat_cols] = df[cat_cols].fillna(\"Unknown\")\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 5. REMOVE DUPLICATES\n",
    "# ===============================================================\n",
    "\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 6. OUTLIER HANDLING (IQR method)\n",
    "# ===============================================================\n",
    "\n",
    "for col in num_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower = Q1 - 1.5 * IQR\n",
    "    upper = Q3 + 1.5 * IQR\n",
    "    df[col] = np.where(df[col] < lower, lower, df[col])\n",
    "    df[col] = np.where(df[col] > upper, upper, df[col])\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 7. NORMALIZE ENGAGEMENT (already normalized, but adding relative forms)\n",
    "# ===============================================================\n",
    "\n",
    "eng_cols = [\"like_rate\", \"comment_rate\", \"share_rate\"]\n",
    "eng_cols = [c for c in eng_cols if c in df.columns]\n",
    "\n",
    "for col in eng_cols:\n",
    "    df[col + \"_zscore\"] = (df[col] - df[col].mean()) / df[col].std()\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 8. FEATURE ENCODING\n",
    "# ===============================================================\n",
    "\n",
    "# One-hot encode key categories\n",
    "onehot_cols = [\"platform\", \"region\", \"language\", \"category\", \"traffic_source\", \"device_brand\"]\n",
    "\n",
    "onehot_cols = [c for c in onehot_cols if c in df.columns]\n",
    "\n",
    "df = pd.get_dummies(df, columns=onehot_cols, drop_first=True)\n",
    "\n",
    "# Label encode the trend label\n",
    "if \"trend_label\" in df.columns:\n",
    "    le = LabelEncoder()\n",
    "    df[\"trend_label_encoded\"] = le.fit_transform(df[\"trend_label\"])\n",
    "# ===== CLEANUP BEFORE STANDARD SCALER =====\n",
    "\n",
    "# Replace inf\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Fill remaining NaNs in numeric columns\n",
    "df[df.select_dtypes(include=[np.number]).columns] = \\\n",
    "    df.select_dtypes(include=[np.number]).fillna(df.median(numeric_only=True))\n",
    "\n",
    "# Remove zero-variance columns\n",
    "zero_var_cols = [col for col in df.select_dtypes(include=[np.number]).columns \n",
    "                 if df[col].nunique() <= 1]\n",
    "\n",
    "print(\"Dropping zero-variance columns:\", zero_var_cols)\n",
    "df = df.drop(columns=zero_var_cols)\n",
    "\n",
    "# Standard scaling\n",
    "scaler = StandardScaler()\n",
    "num_cols = df.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "df[num_cols] = scaler.fit_transform(df[num_cols])\n",
    "\n",
    "\n",
    "# ===============================================================\n",
    "# 9. STANDARD SCALING FOR NUMERIC FEATURES\n",
    "# ===============================================================\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_cols = df.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "\n",
    "df[scaled_cols] = scaler.fit_transform(df[scaled_cols])\n",
    "\n",
    "print(\"Final preprocessed dataset shape:\", df.shape)\n",
    "\n",
    "final_df = df\n",
    "\n",
    "output_path = \"/Users/satvikshankar/Downloads/youtube_tiktok_trends_2025_preprocessed.csv\"\n",
    "\n",
    "final_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"Saved preprocessed dataset to:\", output_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
